{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "name": "HW4_2019517012_노시희.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Qy_S93PpYp_Z",
        "c3B3P_SyYqAy"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SiheeNoh/DFC606/blob/master/HW4_2019517012_%EB%85%B8%EC%8B%9C%ED%9D%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "d7z_MI9zYpek"
      },
      "source": [
        "# HW4 - Data Analysis & Make Prediction Models\n",
        "\n",
        "#### Data Science in Korea University, Prof. Jaewoo Kang\n",
        "#### COSE471, Spring 2020\n",
        "#### Due : 7/01 (wednes)  11:59 PM [Through Black board]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8IE_9O8FYpfZ"
      },
      "source": [
        "## Assignment Credit\n",
        "\n",
        "### Your assignments are 40% of the total score. The scores for each assignment are as follows.\n",
        "- __HW#1: 5%__\n",
        "- __HW#2: 10%__\n",
        "- __HW#3: 10%__\n",
        "- __HW#4: 15%__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OKnzpWp8Ypfx"
      },
      "source": [
        "## Python Assingment HW4 Guide\n",
        "- You need to fill out your code in ###YOUR CODE HERE### or #TODO.\n",
        "- The code results should be the same as the results displayed below the code or the format we gave in problem description.\n",
        "- When solving descriptive problems, both __Korean and English are available.__\n",
        "- The some libraries are already imported below. __But you should import more.__\n",
        "    - If you don't have some libraries, install it with pip or conda command. [Google will help you]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PHGRB5tmYpfz"
      },
      "source": [
        "## Problem1. Scikit-learn\n",
        "- Scikit-learn is the most common python library in machine learning.\n",
        "- Let's port our work from matplotlib to scikit-learn.\n",
        "- However, not all plots will be handled. We will focus on the most recent data, 2010-2015.\n",
        "\n",
        "<b>URL:</b> http://scikit-learn.org/\n",
        "<br/><b>Install guide:</b> http://scikit-learn.org/stable/install.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HyPpqXPWYpf5",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import time\n",
        "from datetime import datetime\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import nltk\n",
        "from sklearn import preprocessing\n",
        "from sklearn import linear_model\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QUPLThvbYpg3"
      },
      "source": [
        "## Problem1. Regression Model\n",
        "## -Amzon stock price prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O806deTZYphI"
      },
      "source": [
        "### Problem 1-1\n",
        "- In this problem you will predict stock price only with price related data. People usually call it as Technical analysis.\n",
        "- Find best combination of parameters to predict next day price.\n",
        "- __Your MSE loss should be lower than 30.__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NdLVRnr3PG8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a10f5c2e-e8db-4d10-86b8-85b34c3f4bd4"
      },
      "source": [
        "%%time\n",
        "!rm -f AMZN.csv covid_all.csv covid_twitter.csv OxCGRT_latest.csv train.csv\n",
        "\n",
        "!wget -nc https://raw.githubusercontent.com/SiheeNoh/DFC606/master/AMZN.csv\n",
        "!wget -nc https://raw.githubusercontent.com/SiheeNoh/DFC606/master/covid_all.csv\n",
        "!wget -nc https://raw.githubusercontent.com/SiheeNoh/DFC606/master/covid_twitter.csv\n",
        "!wget -nc https://raw.githubusercontent.com/SiheeNoh/DFC606/master/OxCGRT_latest.csv\n",
        "!wget -nc https://raw.githubusercontent.com/SiheeNoh/DFC606/master/train.csv"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-01 14:54:21--  https://raw.githubusercontent.com/SiheeNoh/DFC606/master/AMZN.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 375864 (367K) [text/plain]\n",
            "Saving to: ‘AMZN.csv’\n",
            "\n",
            "\rAMZN.csv              0%[                    ]       0  --.-KB/s               \rAMZN.csv            100%[===================>] 367.05K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2020-07-01 14:54:21 (6.37 MB/s) - ‘AMZN.csv’ saved [375864/375864]\n",
            "\n",
            "--2020-07-01 14:54:25--  https://raw.githubusercontent.com/SiheeNoh/DFC606/master/covid_all.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7518135 (7.2M) [text/plain]\n",
            "Saving to: ‘covid_all.csv’\n",
            "\n",
            "covid_all.csv       100%[===================>]   7.17M  16.2MB/s    in 0.4s    \n",
            "\n",
            "2020-07-01 14:54:25 (16.2 MB/s) - ‘covid_all.csv’ saved [7518135/7518135]\n",
            "\n",
            "--2020-07-01 14:54:28--  https://raw.githubusercontent.com/SiheeNoh/DFC606/master/covid_twitter.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7368532 (7.0M) [text/plain]\n",
            "Saving to: ‘covid_twitter.csv’\n",
            "\n",
            "covid_twitter.csv   100%[===================>]   7.03M  15.2MB/s    in 0.5s    \n",
            "\n",
            "2020-07-01 14:54:29 (15.2 MB/s) - ‘covid_twitter.csv’ saved [7368532/7368532]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-67d49c634202>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'!rm -f AMZN.csv covid_all.csv covid_twitter.csv OxCGRT_latest.csv train.csv\\n\\n!wget -nc https://raw.githubusercontent.com/SiheeNoh/DFC606/master/AMZN.csv\\n!wget -nc https://raw.githubusercontent.com/SiheeNoh/DFC606/master/covid_all.csv\\n!wget -nc https://raw.githubusercontent.com/SiheeNoh/DFC606/master/covid_twitter.csv\\n!wget -nc https://raw.githubusercontent.com/SiheeNoh/DFC606/master/OxCGRT_latest.csv\\n!wget -nc https://raw.githubusercontent.com/SiheeNoh/DFC606/master/train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    436\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   result = _run_command(\n\u001b[0;32m--> 438\u001b[0;31m       shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n\u001b[0m\u001b[1;32m    439\u001b[0m   \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_INTERRUPTED_SIGNALS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     with temporary_clearer(), _display_stdin_widget(\n\u001b[0;32m--> 181\u001b[0;31m         delay_millis=500) as update_stdin_widget:\n\u001b[0m\u001b[1;32m    182\u001b[0m       \u001b[0;31m# TODO(b/115531839): Ensure that subprocesses are terminated upon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m       \u001b[0;31m# interrupt.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_display_stdin_widget\u001b[0;34m(delay_millis)\u001b[0m\n\u001b[1;32m    339\u001b[0m   \u001b[0mshell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ipython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m   \u001b[0mdisplay_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'cell_display_stdin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'delayMillis'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdelay_millis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m   \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocking_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdisplay_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_header\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mecho_updater\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_echo_status\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0;31m# unique.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0mrequest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msend_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YkhlNXKMYphN",
        "colab": {}
      },
      "source": [
        "# TODO : load data 'AMZN.csv'\n",
        "#Hint: use pd.read_csv and set read_csv function paramete 'index_col' to set Date  variable to data` Index.\n",
        "amzn = pd.read_csv(\"AMZN.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tblvlKeKYpot",
        "jupyter": {
          "outputs_hidden": false
        },
        "colab": {}
      },
      "source": [
        "amzn.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Nv8uFJ_1Yp6l"
      },
      "source": [
        "<br/><br/>\n",
        "- We will use only __one kind of variable__ for __last 'lookback' days__ to predict next day close price.<br/>\n",
        "For example, if we set lookback days as 5, and use close price for our independent variable.<br/><br/>\n",
        "Close price of [1997-05-15, 1997-05-16, 1997-05-19, 1997-05-20, 1997-05-21] --> predict [1997-05-22] <br/><br/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p6736LRXYp6n",
        "jupyter": {
          "outputs_hidden": true
        },
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "TODO : make get_concated_data function.\n",
        "       this function create x and y data\n",
        "\n",
        "@Function\n",
        "get_concated_data\n",
        "\n",
        "@Inputs\n",
        "__\n",
        "raw_data : dataframe, full Amazon stock data.\n",
        "     \n",
        "\n",
        "@Returns\n",
        "___\n",
        "\n",
        "x_data \n",
        "    : <class 'numpy.ndarray'>\n",
        "    : crteated data should in shape of [len(raw_data)-lookback, lookback]\n",
        "    : x_data should be made based on 'col_name', parameters of the function, variable we will use\n",
        "    \n",
        "y_data \n",
        "    : <class 'numpy.ndarray'>\n",
        "    : crteated data should in shape of [len(raw_data)-lookback]\n",
        "    : y should be always made based on 'Close'.\n",
        "\"\"\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-iMOtjtz_ACz",
        "colab": {}
      },
      "source": [
        "amzn.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5vJ46JDpYp7N",
        "colab": {}
      },
      "source": [
        "# We will use only one kind of variable as a time series data\n",
        "def get_concated_data(raw_data, col_name, lookback):\n",
        "    x_datas = list()\n",
        "    y_datas = list()\n",
        "    # col_name : Variable to be used\n",
        "    col_data = raw_data[col_name].values\n",
        "    # aggregate target variable data for length of lookback days\n",
        "    \n",
        "    ##### Your Code Here ####\n",
        "    ##HInt: use for function and append function in for function x,y data to x_datas,y_datas\n",
        "    \n",
        "    # crteated data should in shape of [len(raw_data)-lookback, lookback]\n",
        "    # x_data should be made based on 'col_name', parameters of the function, variable we will use\n",
        "    x_datas = np.zeros([len(raw_data)-lookback, lookback])\n",
        "\n",
        "    for i in range(0, lookback):\n",
        "        x_datas[:, i] = raw_data[col_name].shift(lookback-i)[lookback:len(raw_data)].values\n",
        "        #x_datas.append(raw_data[col_name][i-lookback:i])\n",
        "    y_datas = raw_data['Close'][-(len(raw_data)-lookback):]\n",
        "    #########################    \n",
        "    x_data = np.concatenate(x_datas,0)\n",
        "    y_data = np.asarray(y_datas)\n",
        "\n",
        "    return x_data, y_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C3_avW6JYp7f",
        "jupyter": {
          "outputs_hidden": true
        },
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "TODO : make split_data function which split train / test (last 60 data, approximately 3 month)\n",
        "\n",
        "@Function\n",
        "split_datas\n",
        "\n",
        "@Inputs\n",
        "__\n",
        "data : numpy array\n",
        "     \n",
        "@Returns\n",
        "___\n",
        "x_train, x_test, y_train, y_test\n",
        "    :<class 'numpy.ndarray'>\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QxMgjl54Yp9M",
        "jupyter": {
          "outputs_hidden": true
        },
        "colab": {}
      },
      "source": [
        "def split_datas(x,y):\n",
        "    ##### Your Code Here ####\n",
        "    ##hint: for time series data yo should not split data randomly ,just split data '0:your split parameter' , 'split parameter: last'\n",
        "    ratio = int(len(x) / len(y))\n",
        "    x_train = x[:-60*ratio]\n",
        "    y_train = y[:-60]\n",
        "    x_test = x[-60*ratio:]\n",
        "    y_test = y[-60:]\n",
        "    ###########################\n",
        "    return x_train, x_test, y_train, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "481lJJGDYp9g",
        "jupyter": {
          "outputs_hidden": false
        },
        "colab": {}
      },
      "source": [
        "x, y = get_concated_data(amzn, 'Close', 5)\n",
        "x[:10], y[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IaBvyY7QYp9t",
        "jupyter": {
          "outputs_hidden": false
        },
        "colab": {}
      },
      "source": [
        "x_train, x_test, y_train, y_test = split_datas(x, y)\n",
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lOw6WL1aYp9_"
      },
      "source": [
        "### <br/>Problem 1-2 Find best parameter combination \n",
        "- You can choose which __columns to use__ as variable for __how many days__, and in which __polynomial space.__\n",
        "- Find the best combination of parameters to predict next day price.\n",
        "- __Find combination which makes MSE loss for train data lower than 6.__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jIK3NpnZYp-H",
        "jupyter": {
          "outputs_hidden": true
        },
        "colab": {}
      },
      "source": [
        "# TODO : complete run_regression function\n",
        "# Hint : use api i gave below"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tKjUnGuoYp-h",
        "jupyter": {
          "outputs_hidden": true
        },
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn import linear_model\n",
        "\n",
        "def run_regression(raw_data, column_name, lookback_days, poly_degree):\n",
        "    #print(column_name, lookback_days, poly_degree)\n",
        "    # performing regression analysis with specified parameters\n",
        "    x, y = get_concated_data(raw_data, column_name, lookback_days)\n",
        "    x_train, x_test, y_train, y_test = split_datas(x, y)\n",
        "    \n",
        "    ##### Your Code Here ####\n",
        "    PolynomialRegression = PolynomialFeatures(degree = poly_degree)\n",
        "    poly = PolynomialRegression.fit_transform(np.reshape(x, (len(y), lookback_days)))\n",
        "\n",
        "    X_train = PolynomialRegression.fit_transform(np.reshape(x_train, (len(y_train), lookback_days)))\n",
        "    X_test = PolynomialRegression.fit_transform(np.reshape(x_test, (len(y_test), lookback_days)))\n",
        "    \n",
        "    model = LinearRegression().fit(X_train, y_train) #fit the model\n",
        "    train_pred = model.predict(X_train) # for train which data you have to use?\n",
        "    test_pred = model.predict(X_test) # for test which data you have to use?\n",
        "    \n",
        "    print (\"MSE - Train %.4f\"%mean_squared_error(model.predict(X_train), y_train))\n",
        "    print (\"MSE - Test %.4f\"%mean_squared_error(model.predict(X_test), y_test))\n",
        "    \n",
        "    return test_pred, y_test, mean_squared_error(model.predict(X_train), y_train), mean_squared_error(model.predict(X_test), y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yinFvYu_44oS",
        "colab": {}
      },
      "source": [
        "amzn.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_pcQlZ6fYp-_",
        "jupyter": {
          "outputs_hidden": true
        },
        "colab": {}
      },
      "source": [
        "# TODO : run experiment with various combination\n",
        "best_pred = 0\n",
        "best_test = 0\n",
        "best_column = ''\n",
        "best_lookback_day = 0\n",
        "best_poly_degree = 0\n",
        "best_error = 0\n",
        "best_list = []\n",
        "\n",
        "for lookback_day in range(10, 15):\n",
        "    if len(best_list) > 0:\n",
        "        break\n",
        "\n",
        "    for column in amzn.columns[1:]:\n",
        "        if len(best_list) > 0:\n",
        "            break\n",
        "            \n",
        "        for poly_degree in range(1, 5):            \n",
        "            test_pred, y_test, train_error, test_error = run_regression(amzn, column_name=column, lookback_days=lookback_day, poly_degree=poly_degree)\n",
        "            if train_error < 6:\n",
        "                best_column = column\n",
        "                best_lookback_day = lookback_day\n",
        "                best_poly_degree = poly_degree\n",
        "                best_pred = test_pred\n",
        "                best_error = test_error\n",
        "                best_list.append([ column, lookback_day, poly_degree, train_error, test_error])\n",
        "                break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ThcnkyWi7kPZ",
        "colab": {}
      },
      "source": [
        " best_column, best_lookback_day, best_poly_degree"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aY8NSh06Yp_R"
      },
      "source": [
        "Write Combination that you've found. (Train loss lower than 6) <br/><br/>\n",
        "Variable :    Close   <br/>\n",
        "Lookback :    11    <br/>\n",
        "Poly_degree :  4     <br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Qy_S93PpYp_Z"
      },
      "source": [
        "### Problem 1-3 Overfitting & Underfitting\n",
        "1. Briefly explain about the concept of overfitting and underftting with proper example that you can find from experiment above.\n",
        "2. In the above problem desciption, I set your goal as fiding best parameters for total data loss. What is wrong with it? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-z8u8GJ6Yp_b"
      },
      "source": [
        "### YOUR ANSWER HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5NCNrfkBYp_c"
      },
      "source": [
        "- 1. Train Error가 0.80, Test Error 52713042603.32 로 학습된 모델의 오차는 적어도 테스트 데이터의 에러 크기 워낙 커서 오버피팅 되었다.\n",
        "- 2. Train Error가 아닌 Test Error가 적게 나오는 파라미터를 찾는 로직으로 수정해야 한다.\n",
        "      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DiZzYKFdYp_e"
      },
      "source": [
        "### Problem 1-4 Visualization\n",
        "- Visualize your predicton.\n",
        "- You can use code below with proper variable.\n",
        "- __Your best model's prediction --> best_pred.__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l7J5o5DI8hmB",
        "colab": {}
      },
      "source": [
        "# TODO : run experiment with various combination\n",
        "best_pred = 0\n",
        "best_test = 0\n",
        "best_column = ''\n",
        "best_lookback_day = 0\n",
        "best_poly_degree = 0\n",
        "best_error = -1\n",
        "best_list = []\n",
        "\n",
        "for lookback_day in range(1, 15):\n",
        "    #for column in raw_data.columns[1:]:\n",
        "    for column in ['Close', 'Adj Close']:\n",
        "        for poly_degree in range(1, 5):            \n",
        "            test_pred, y_test, error, test_error = run_regression(amzn, column_name=column, lookback_days=lookback_day, poly_degree=poly_degree)\n",
        "            if best_error < 0 or test_error < best_error:\n",
        "                best_column = column\n",
        "                best_lookback_day = lookback_day\n",
        "                best_poly_degree = poly_degree\n",
        "                best_pred = test_pred\n",
        "                best_error = test_error\n",
        "                best_list.append([ column, lookback_day, poly_degree, error, test_error])\n",
        "                #break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xesG-sNPYp_g",
        "colab": {}
      },
      "source": [
        "# TODO : Assign your best prediction for test data to best_pred\n",
        "print('Variable:', best_column)\n",
        "print('Lookback:', best_lookback_day)\n",
        "print('Poly_degree:', best_poly_degree)\n",
        "\n",
        "best_pred, y_besty, b_error, best_test_error = run_regression(raw_data, best_column, best_lookback_day, best_poly_degree)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oYND4tRqYp_n",
        "jupyter": {
          "outputs_hidden": false
        },
        "colab": {}
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.plot(best_pred, label='prediction')\n",
        "plt.plot(y_test, label='target')\n",
        "plt.legend(loc='best', fontsize=20)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OoEebgnVYp_w"
      },
      "source": [
        "## Problem2. Regression Model\n",
        "## -Covid Confirmed prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "veaSPN9fYp_2",
        "colab": {}
      },
      "source": [
        "# #import covidall dataset \n",
        "# train = pd.read_csv('../train.csv')\n",
        "# all_data = pd.read_csv('../covid_all.csv') #set your working directory\n",
        "# all_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o6Xnc6GS6NHA",
        "colab": {}
      },
      "source": [
        "#import covidall dataset \n",
        "train = pd.read_csv(\"train.csv\", sep=\",\")\n",
        "all_data = pd.read_csv(\"covid_all.csv\", sep=\",\")\n",
        "all_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FuwEPxDpYp__",
        "colab": {}
      },
      "source": [
        "country_dict = {'Afghanistan': 0,\n",
        " 'Albania': 1,\n",
        " 'Algeria': 2,\n",
        " 'Andorra': 3,\n",
        " 'Angola': 4,\n",
        " 'Antigua and Barbuda': 5,\n",
        " 'Argentina': 6,\n",
        " 'Armenia': 7,\n",
        " 'Australia': 8,\n",
        " 'Austria': 9,\n",
        " 'Azerbaijan': 10,\n",
        " 'Bahamas': 11,\n",
        " 'Bahrain': 12,\n",
        " 'Bangladesh': 13,\n",
        " 'Barbados': 14,\n",
        " 'Belarus': 15,\n",
        " 'Belgium': 16,\n",
        " 'Belize': 17,\n",
        " 'Benin': 18,\n",
        " 'Bhutan': 19,\n",
        " 'Bolivia': 20,\n",
        " 'Bosnia and Herzegovina': 21,\n",
        " 'Botswana': 22,\n",
        " 'Brazil': 23,\n",
        " 'Brunei': 24,\n",
        " 'Bulgaria': 25,\n",
        " 'Burkina Faso': 26,\n",
        " 'Burma': 27,\n",
        " 'Burundi': 28,\n",
        " 'Cabo Verde': 29,\n",
        " 'Cambodia': 30,\n",
        " 'Cameroon': 31,\n",
        " 'Canada': 32,\n",
        " 'Central African Republic': 33,\n",
        " 'Chad': 34,\n",
        " 'Chile': 35,\n",
        " 'China': 36,\n",
        " 'Colombia': 37,\n",
        " 'Congo (Brazzaville)': 38,\n",
        " 'Congo (Kinshasa)': 39,\n",
        " 'Costa Rica': 40,\n",
        " \"Cote d'Ivoire\": 41,\n",
        " 'Croatia': 42,\n",
        " 'Cuba': 43,\n",
        " 'Cyprus': 44,\n",
        " 'Czechia': 45,\n",
        " 'Denmark': 46,\n",
        " 'Diamond Princess': 47,\n",
        " 'Djibouti': 48,\n",
        " 'Dominica': 49,\n",
        " 'Dominican Republic': 50,\n",
        " 'Ecuador': 51,\n",
        " 'Egypt': 52,\n",
        " 'El Salvador': 53,\n",
        " 'Equatorial Guinea': 54,\n",
        " 'Eritrea': 55,\n",
        " 'Estonia': 56,\n",
        " 'Eswatini': 57,\n",
        " 'Ethiopia': 58,\n",
        " 'Fiji': 59,\n",
        " 'Finland': 60,\n",
        " 'France': 61,\n",
        " 'Gabon': 62,\n",
        " 'Gambia': 63,\n",
        " 'Georgia': 64,\n",
        " 'Germany': 65,\n",
        " 'Ghana': 66,\n",
        " 'Greece': 67,\n",
        " 'Grenada': 68,\n",
        " 'Guatemala': 69,\n",
        " 'Guinea': 70,\n",
        " 'Guinea-Bissau': 71,\n",
        " 'Guyana': 72,\n",
        " 'Haiti': 73,\n",
        " 'Holy See': 74,\n",
        " 'Honduras': 75,\n",
        " 'Hungary': 76,\n",
        " 'Iceland': 77,\n",
        " 'India': 78,\n",
        " 'Indonesia': 79,\n",
        " 'Iran': 80,\n",
        " 'Iraq': 81,\n",
        " 'Ireland': 82,\n",
        " 'Israel': 83,\n",
        " 'Italy': 84,\n",
        " 'Jamaica': 85,\n",
        " 'Japan': 86,\n",
        " 'Jordan': 87,\n",
        " 'Kazakhstan': 88,\n",
        " 'Kenya': 89,\n",
        " 'Korea, South': 90,\n",
        " 'Kosovo': 91,\n",
        " 'Kuwait': 92,\n",
        " 'Kyrgyzstan': 93,\n",
        " 'Laos': 94,\n",
        " 'Latvia': 95,\n",
        " 'Lebanon': 96,\n",
        " 'Liberia': 97,\n",
        " 'Libya': 98,\n",
        " 'Liechtenstein': 99,\n",
        " 'Lithuania': 100,\n",
        " 'Luxembourg': 101,\n",
        " 'MS Zaandam': 102,\n",
        " 'Madagascar': 103,\n",
        " 'Malawi': 104,\n",
        " 'Malaysia': 105,\n",
        " 'Maldives': 106,\n",
        " 'Mali': 107,\n",
        " 'Malta': 108,\n",
        " 'Mauritania': 109,\n",
        " 'Mauritius': 110,\n",
        " 'Mexico': 111,\n",
        " 'Moldova': 112,\n",
        " 'Monaco': 113,\n",
        " 'Mongolia': 114,\n",
        " 'Montenegro': 115,\n",
        " 'Morocco': 116,\n",
        " 'Mozambique': 117,\n",
        " 'Namibia': 118,\n",
        " 'Nepal': 119,\n",
        " 'Netherlands': 120,\n",
        " 'New Zealand': 121,\n",
        " 'Nicaragua': 122,\n",
        " 'Niger': 123,\n",
        " 'Nigeria': 124,\n",
        " 'North Macedonia': 125,\n",
        " 'Norway': 126,\n",
        " 'Oman': 127,\n",
        " 'Pakistan': 128,\n",
        " 'Panama': 129,\n",
        " 'Papua New Guinea': 130,\n",
        " 'Paraguay': 131,\n",
        " 'Peru': 132,\n",
        " 'Philippines': 133,\n",
        " 'Poland': 134,\n",
        " 'Portugal': 135,\n",
        " 'Qatar': 136,\n",
        " 'Romania': 137,\n",
        " 'Russia': 138,\n",
        " 'Rwanda': 139,\n",
        " 'Saint Kitts and Nevis': 140,\n",
        " 'Saint Lucia': 141,\n",
        " 'Saint Vincent and the Grenadines': 142,\n",
        " 'San Marino': 143,\n",
        " 'Sao Tome and Principe': 144,\n",
        " 'Saudi Arabia': 145,\n",
        " 'Senegal': 146,\n",
        " 'Serbia': 147,\n",
        " 'Seychelles': 148,\n",
        " 'Sierra Leone': 149,\n",
        " 'Singapore': 150,\n",
        " 'Slovakia': 151,\n",
        " 'Slovenia': 152,\n",
        " 'Somalia': 153,\n",
        " 'South Africa': 154,\n",
        " 'South Sudan': 155,\n",
        " 'Spain': 156,\n",
        " 'Sri Lanka': 157,\n",
        " 'Sudan': 158,\n",
        " 'Suriname': 159,\n",
        " 'Sweden': 160,\n",
        " 'Switzerland': 161,\n",
        " 'Syria': 162,\n",
        " 'Taiwan*': 163,\n",
        " 'Tanzania': 164,\n",
        " 'Thailand': 165,\n",
        " 'Timor-Leste': 166,\n",
        " 'Togo': 167,\n",
        " 'Trinidad and Tobago': 168,\n",
        " 'Tunisia': 169,\n",
        " 'Turkey': 170,\n",
        " 'US': 171,\n",
        " 'Uganda': 172,\n",
        " 'Ukraine': 173,\n",
        " 'United Arab Emirates': 174,\n",
        " 'United Kingdom': 175,\n",
        " 'Uruguay': 176,\n",
        " 'Uzbekistan': 177,\n",
        " 'Venezuela': 178,\n",
        " 'Vietnam': 179,\n",
        " 'West Bank and Gaza': 180,\n",
        " 'Western Sahara': 181,\n",
        " 'Zambia': 182,\n",
        " 'Zimbabwe': 183}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Vf6J-Tc8YqAF"
      },
      "source": [
        "### <br/>Problem 2-1 Plot ' Spain' covid-Confiremedcases graph between march 1 to march 11.\n",
        "- your completed graph doesn't have to be the same as the example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ckJ7mAfwYqAP"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VtsSWZo56rsV",
        "colab": {}
      },
      "source": [
        "# plot ' Spain' covid-Confiremedcases graph between march 1 to march 11.\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,6))\n",
        "y1 = all_data[(all_data['Country_Region']==country_dict['Spain']) & (all_data['Day_num']>39) & (all_data['Day_num']<=49)][['ConfirmedCases']]\n",
        "x1 = range(0, len(y1))\n",
        "ax1.plot(x1, y1, 'bo--')\n",
        "ax1.set_title(\"Spain ConfirmedCases between days 39 and 49\")\n",
        "ax1.set_xlabel(\"Days\")\n",
        "ax1.set_ylabel(\"ConfirmedCases\")\n",
        "\n",
        "## To do ###\n",
        "# Hint : using the above process but you have to transform your variable to log scale.\n",
        "##apply log transform to y variables ## \n",
        "x2 = x1\n",
        "y2 = np.log(y1)\n",
        "ax2.plot(x2, y2, 'bo--')\n",
        "ax2.set_title(\"Spain ConfirmedCases between days 39 and 49\")\n",
        "ax2.set_xlabel(\"Days\")\n",
        "ax2.set_ylabel(\"ConfirmedCases\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N1U13kHjYqAQ",
        "colab": {}
      },
      "source": [
        "##don`t touch this cell##\n",
        "\n",
        "# Filter selected features\n",
        "data = all_data.copy()\n",
        "features = ['Id', 'ForecastId', 'Country_Region', 'Province_State', 'ConfirmedCases', 'Fatalities', \n",
        "       'Day_num']\n",
        "data = data[features]\n",
        "\n",
        "# Apply log transformation to all ConfirmedCases and Fatalities columns, except for trends\n",
        "data[['ConfirmedCases', 'Fatalities']] = data[['ConfirmedCases', 'Fatalities']].astype('float64')\n",
        "data[['ConfirmedCases', 'Fatalities']] = data[['ConfirmedCases', 'Fatalities']].apply(lambda x: np.log1p(x))\n",
        "\n",
        "# Replace infinites\n",
        "data.replace([np.inf, -np.inf], 0, inplace=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3ImRTVLQYqAZ"
      },
      "source": [
        "### <br/>Problem 2-2  complete below function to  Split data into train/test and apply linear regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LrIO31qvYqAa",
        "colab": {}
      },
      "source": [
        "#  Split data into train/test\n",
        "def split_data(df, train_lim, test_lim):\n",
        "    df.loc[df['Day_num']<=train_lim , 'ForecastId'] = -1\n",
        "    df = df[df['Day_num']<=test_lim]\n",
        "    \n",
        "    ##To do ## for Train set \n",
        "    x_train = df[(df['Day_num']<=train_lim)].drop(['ConfirmedCases', 'Fatalities'], axis=1)\n",
        "    y_train_1 = df.loc[(df['Day_num']<=train_lim), 'ConfirmedCases']\n",
        "    y_train_2 = df.loc[(df['Day_num']<=train_lim), 'Fatalities']\n",
        "    \n",
        "    ##To do ## for  Test set\n",
        "    x_test = df[(df['Day_num']>train_lim)].drop(['ConfirmedCases', 'Fatalities'], axis=1)\n",
        "    \n",
        "    # Clean Id columns and keep ForecastId as index\n",
        "    x_train.drop('Id', inplace=True, errors='ignore', axis=1)\n",
        "    x_train.drop('ForecastId', inplace=True, errors='ignore', axis=1)\n",
        "\n",
        "    x_test.drop('Id', inplace=True, errors='ignore', axis=1)\n",
        "    x_test.drop('ForecastId', inplace=True, errors='ignore', axis=1)\n",
        "\n",
        "    return x_train, y_train_1, y_train_2, x_test #check to devide y_dataset \n",
        "\n",
        "# Linear regression model\n",
        "def lin_reg(X_train, Y_train, X_test):\n",
        "    ##To do ##   Create linear regression object\n",
        "    regr = linear_model.LinearRegression()\n",
        "    \n",
        "    ## To do ##  Train the model using the training sets\n",
        "    regr.fit(X_train, Y_train)\n",
        "    \n",
        "    # To do ## Make predictions using the testing set\n",
        "    y_pred = regr.predict(X_test)\n",
        "    \n",
        "    return regr, y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Bq3bGqZMYqAg",
        "colab": {}
      },
      "source": [
        "##don`t touch this cell##\n",
        "dates_list = ['2020-03-01', '2020-03-02', '2020-03-03', '2020-03-04', '2020-03-05', '2020-03-06', '2020-03-07', '2020-03-08', '2020-03-09', \n",
        "                 '2020-03-10', '2020-03-11','2020-03-12','2020-03-13','2020-03-14','2020-03-15','2020-03-16','2020-03-17','2020-03-18',\n",
        "                 '2020-03-19','2020-03-20','2020-03-21','2020-03-22','2020-03-23', '2020-03-24', '2020-03-25', '2020-03-26', '2020-03-27', \n",
        "                 '2020-03-28', '2020-03-29', '2020-03-30', '2020-03-31', '2020-04-01', '2020-04-02', '2020-04-03', '2020-04-04', '2020-04-05', \n",
        "                 '2020-04-06', '2020-04-07', '2020-04-08', '2020-04-09', '2020-04-10', '2020-04-11', '2020-04-12', '2020-04-13', '2020-04-14','2020-04-15','2020-04-16','2020-04-17','2020-04-18','2020-04-19',\n",
        "                '2020-04-20','2020-04-21','2020-04-22','2020-04-23','2020-04-24','2020-04-25','2020-04-26','2020-04-27','2020-04-28','2020-04-29','2020-04-30',\n",
        "                '2020-05-01','2020-05-02','2020-05-03','2020-05-04','2020-05-05','2020-05-06','2020-05-07','2020-05-08','2020-05-09','2020-05-10','2020-05-11',\n",
        "                '2020-05-12','2020-05-13', '2020-05-14']\n",
        "\n",
        "def plot_linreg_basic_country(data, country_name, dates_list, day_start, shift, train_lim, test_lim):\n",
        "    \n",
        "    data_country = data[data['Country_Region']==country_dict[country_name]]\n",
        "    data_country = data_country.loc[data_country['Day_num']>=day_start]\n",
        "    X_train, Y_train_1, Y_train_2, X_test = split_data(data_country, train_lim, test_lim)\n",
        "    model, pred = lin_reg(X_train, Y_train_1, X_test)\n",
        "    \n",
        "    # Create a df with both real cases and predictions (predictions starting on March 12th)\n",
        "    X_train_check = X_train.copy()\n",
        "    X_train_check['Target'] = Y_train_1\n",
        "    \n",
        "    X_test_check = X_test.copy()\n",
        "    X_test_check['Target'] = pred\n",
        "    \n",
        "    X_final_check = pd.concat([X_train_check, X_test_check])\n",
        "    \n",
        "    # Select predictions from March 1st to March 25th\n",
        "    predicted_data = X_final_check.loc[(X_final_check['Day_num'].isin(list(range(day_start, day_start+len(dates_list)))))].Target\n",
        "    real_data = train.loc[(train['Country_Region']==country_name) & (train['Date'].isin(dates_list))]['ConfirmedCases']\n",
        "    dates_list_num = list(range(0,len(dates_list)))\n",
        "    \n",
        "    # Plot results\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,6))\n",
        "    \n",
        "    ax1.plot(dates_list_num, np.expm1(predicted_data))\n",
        "    ax1.plot(dates_list_num, real_data)\n",
        "    ax1.axvline(30-shift, linewidth=2, ls = ':', color='grey', alpha=0.5)\n",
        "    \n",
        "    ax1.legend(['Predicted cases', 'Actual cases', 'Train-test split'], loc='upper left')\n",
        "    ax1.set_xlabel(\"Day count (from March \" + str(1+shift) + \" to March 25th)\")\n",
        "    ax1.set_ylabel(\"Confirmed Cases\")\n",
        "    \n",
        "    ax2.plot(dates_list_num, predicted_data)\n",
        "    ax2.plot(dates_list_num, np.log1p(real_data))\n",
        "    ax2.axvline(30-shift, linewidth=2, ls = ':', color='grey', alpha=0.5)\n",
        "    ax2.legend(['Predicted cases', 'Actual cases', 'Train-test split'], loc='upper left')\n",
        "    ax2.set_xlabel(\"Day count (from March \" + str(1+shift) + \" to March 30th)\")\n",
        "    ax2.set_ylabel(\"Log Confirmed Cases\")\n",
        "\n",
        "    plt.suptitle((\"ConfirmedCases predictions based on Log-Lineal Regression for \"+country_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1BUt-r4GYqAq"
      },
      "source": [
        "### <br/>Problem 2-3  using above function'plot_linreg_basic_country' run the Linear Regression workflow in 'Korea, South' case\n",
        " - your completed graph doesn't have to be the same as the example.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VQTyDs2rYqAr",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "## To do ##  \n",
        "country_name = 'Korea, South'\n",
        "march_day = 1\n",
        "day_start = 39\n",
        "dates_list2 = dates_list[march_day:]\n",
        "train_lim, test_lim = 69, 112\n",
        "plot_linreg_basic_country(data, country_name, dates_list2, day_start, march_day, train_lim, test_lim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "P6wZ11gEYqAx"
      },
      "source": [
        "굵은 텍스트![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "c3B3P_SyYqAy"
      },
      "source": [
        "### <br/>Problem 2-4 Describe the significance and limitations of the graph above.\n",
        "\n",
        "- Day Count가 30미만에서 데이터를 잘 학습하다가 그 이후 에러가 크게 증가한다.\n",
        "  즉, 오버피팅 문제가 발생함\n",
        "  \n",
        "- 오버피팅 해결방안\n",
        "   1. 더 많은 데이터셋으로 모델링\n",
        "   2. 피처수를 작게하여 테스트 에러를 줄이는 모델을 찾는다.\n",
        "   3. 학습 기간을 늘린다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PaKsuRZAkyeG"
      },
      "source": [
        "![대체 텍스트](https://)학."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4kaYz2bcYqA9"
      },
      "source": [
        "## 3. Caculating Stringency Indexes of six countries \n",
        "\n",
        "Reference : https://www.bsg.ox.ac.uk/sites/default/files/Calculation%20and%20presentation%20of%20the%20Stringency%20Index.pdf\n",
        "\n",
        "Look at the document above and learn about Stringency Index (not legacy stringency index!!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nOiuFGMUYqA-"
      },
      "source": [
        "## 3-1 Data filtering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F40l6p0xYqA_",
        "colab": {}
      },
      "source": [
        "#load csv file you care (\"\")\n",
        "###YOUR CODE HERE###\n",
        "latest = pd.read_csv(\"OxCGRT_latest.csv\", sep=\",\")\n",
        "latest.to_csv(\"data.csv\", sep=\",\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6GxG0BgyYqBE",
        "colab": {}
      },
      "source": [
        "#Filter rows with six unique countries by 'CountryName'\n",
        "#('CountryName' == Australia, China, Japan, New Zealand, Singapore, South Korea)\n",
        "###YOUR CODE HERE###\n",
        "OxCGRT =  latest[latest['CountryName'].isin(['Australia', 'China', 'Japan', 'New Zealand', 'Singapore', 'South Korea'])]\n",
        "OxCGRT.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dflX0FpeYqBK",
        "colab": {}
      },
      "source": [
        "#Drop columns you don't need\n",
        "#print dataframe you care about\n",
        "###YOUR CODE HERE###\n",
        "#use columns : \n",
        "\n",
        "columns = ['CountryName', 'Date', 'C1_School closing', 'C1_Flag', 'C2_Workplace closing', 'C2_Flag', 'C3_Cancel public events','C3_Flag',\n",
        "        'C4_Restrictions on gatherings','C4_Flag','C5_Close public transport','C5_Flag','C6_Stay at home requirements','C6_Flag',\n",
        "        'C7_Restrictions on internal movement','C7_Flag','C8_International travel controls','H1_Public information campaigns','H1_Flag']\n",
        "\n",
        "OxCGRT = OxCGRT[columns]\n",
        "OxCGRT = OxCGRT.reset_index(drop=True)\n",
        "OxCGRT.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ukTFRZ3eYqBQ"
      },
      "source": [
        "## 3-2 Calculating Stringency Index\n",
        "\n",
        "you can use some pakages to deal with 'nan' values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ea4fLMgw-pWY",
        "colab": {}
      },
      "source": [
        "OxCGRT.fillna(0, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jvP1rVv3YqBS",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "#stringency calculation\n",
        "#step1 : calculate weight for a policy of general scope\n",
        "###YOUR CODE HERE###\n",
        "OxCGRT[\"W\"] = 0\n",
        "\n",
        "for i in range(0, len(OxCGRT)):\n",
        "    result = 0\n",
        "    for column in ['C1_School closing', 'C2_Workplace closing', 'C3_Cancel public events',\n",
        "                    'C4_Restrictions on gatherings', 'C5_Close public transport',\n",
        "                    'C6_Stay at home requirements',  'C7_Restrictions on internal movement',\n",
        "                    'H1_Public information campaigns']:\n",
        "        result += 1/(OxCGRT.loc[i, column]+1)/8\n",
        "        #print(result)\n",
        "    OxCGRT.loc[i, \"W\"] = result\n",
        "    #print(data.loc[i, \"W\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aMFEnYfPBtoJ",
        "colab": {}
      },
      "source": [
        "OxCGRT.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nJfyHfk6YqBb",
        "colab": {}
      },
      "source": [
        "#step2 : calculate sub-indices\n",
        "#calculate C1-C9, G1-G8, and I1-I9 \n",
        "###YOUR CODE HERE###\n",
        "OxCGRT['C1'] = OxCGRT['C1_School closing']\n",
        "OxCGRT['C2'] = OxCGRT['C2_Workplace closing']\n",
        "OxCGRT['C3'] = OxCGRT['C3_Cancel public events']\n",
        "OxCGRT['C4'] = OxCGRT['C4_Restrictions on gatherings']\n",
        "OxCGRT['C5'] = OxCGRT['C5_Close public transport']\n",
        "OxCGRT['C6'] = OxCGRT['C6_Stay at home requirements']\n",
        "OxCGRT['C7'] = OxCGRT['C7_Restrictions on internal movement']\n",
        "OxCGRT['C8'] = OxCGRT['C8_International travel controls']\n",
        "OxCGRT['C9'] = OxCGRT['H1_Public information campaigns']\n",
        "OxCGRT['C9_Flag'] = OxCGRT['H1_Flag']\n",
        "\n",
        "CFlgas = [\"C1_Flag\", \"C2_Flag\", \"C3_Flag\", \"C4_Flag\", \"C5_Flag\", \"C6_Flag\", \"C7_Flag\", \"H1_Flag\"]\n",
        "\n",
        "#numbers = range(1, 9)\n",
        "Ns = [3,3,2,4,2,4,2,2]\n",
        "for i in range(1, 9):\n",
        "    OxCGRT['G'+str(i+1)] = 1\n",
        "    OxCGRT['I'+str(i+1)] = 0.0       \n",
        "    \n",
        "for i in range(0, len(OxCGRT)):\n",
        "    result = 0\n",
        "        \n",
        "    for j in range(0, 8):        \n",
        "        w = OxCGRT.loc[i, \"W\"]\n",
        "        C = OxCGRT.loc[i, CFlgas[j]]\n",
        "        N = Ns[j]\n",
        "        G = OxCGRT.loc[i, \"G\"+str(j+1)]\n",
        "        I = 100 * (  C * (1-w) / N + w * G)\n",
        "        \n",
        "        OxCGRT.loc[i, \"I\"+str(j+1)] = I\n",
        "\n",
        "OxCGRT['I9'] = 100 * OxCGRT['C8_International travel controls'] / 4 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fO8a1PIzYqBl",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "#step3 : calculate stringency index I (up to 2 decimal points)\n",
        "###YOUR CODE HERE###\n",
        "stringency_index = (OxCGRT[['I1', 'I2', 'I3', 'I4', 'I5', 'I6', 'I7','I8', 'I9']]).mean(axis=1)\n",
        "OxCGRT['NULL'] = OxCGRT[['I1', 'I2', 'I3', 'I4', 'I5', 'I6', 'I7','I8', 'I9']].isnull().sum(axis=1)\n",
        "stringency_index = round(stringency_index, 2)\n",
        "stringency_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B7GIJfXbYqBq",
        "colab": {}
      },
      "source": [
        "#step4 : make your stingency index array into dataframe(column name = 'my_StringencyIndex') and concat it(axis = 1) to the result of 1-1 \n",
        "###YOUR CODE HERE###\n",
        "OxCGRT['StringencyIndex'] = OxCGRT6['StringencyIndex']\n",
        "OxCGRT.loc[OxCGRT['NULL'] < 6, \"my_StringencyIndex\"] = stringency_index\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x9QN22t4cxgH"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KrdIixqmYqBu"
      },
      "source": [
        "## Problem4. Topic Modeling (Clustering analysis of documents) for tweets related to COVID-19\n",
        "A **topic model** is a type of statistical model for discovering the abstract \"topics\" that occur in a collection of documents.\n",
        " Please refer to [wikipedia](https://en.wikipedia.org/wiki/Topic_model#:~:text=In%20machine%20learning%20and%20natural,structures%20in%20a%20text%20body.) for detailed information.\n",
        "- These documents were crawled from *twitter* with some *COVID-19* related keywords . All tweets are in English but have a lot of noisy information.\n",
        "- In this problem, your goal is to cluster tweets according to the latent 'topics' with the statistical method, automatically. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VylF0FXCYqBv"
      },
      "source": [
        "### Load the csv file using pandas library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MbStL94TYqBw",
        "colab": {}
      },
      "source": [
        "#df = pd.read_csv('../data/covid_twitter.csv') #set your working directory\n",
        "df = pd.read_csv(\"covid_twitter.csv\", sep=\",\")\n",
        "\n",
        "text_en = df['text']\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cpvGiHiuYqB3"
      },
      "source": [
        "### Problem 4-1 Text Preprocessing\n",
        "First of all, you should process the documents so that the model will not be perturbed by the noisy information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AC04bge7YqB4"
      },
      "source": [
        "**a. Remove the following characters with *'re'* module in each tweets**\n",
        "- special characters ( # @ , . | ? ! )\n",
        "- *COVID* relavant words (COVID19, coronavirus, coronaoutbreak)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ggpDFv84YqB5",
        "colab": {}
      },
      "source": [
        "### YOUR CODE HERE ###\n",
        "#Hint. repeat first sentence process but change parameter to fit your gal\n",
        "text_en = text_en.apply(lambda x: re.sub(r\"[\\#\\@\\,\\.\\d\\'\\|\\(\\)\\-]\", \"\", str(x)))\n",
        "text_en = text_en.apply(lambda x: re.sub(\"COVID|COVID19|coronavirus|coronaoutbreak|covid|corona|Corona|Covid\", \"\", str(x)))\n",
        "text_en[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3SR2kJgkYqCA"
      },
      "source": [
        "**b. Remove the *stop words* in each tweets**\n",
        "\n",
        "*Stop words* are words which should be filtered out before given to the model. Please refer to [wikipedia](https://en.wikipedia.org/wiki/Stop_words#:~:text=In%20SEO%20terminology%2C%20stop%20words,save%20space%20in%20their%20databases.) for detailed information \n",
        "- First, you should download the stop word set from *nltk* module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "46gF35bAYqCB",
        "colab": {}
      },
      "source": [
        "### NLP toolkit ###\n",
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uF8tQtCMYqCH",
        "colab": {}
      },
      "source": [
        "### Download the set with this\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "josTnbBvYqCR",
        "colab": {}
      },
      "source": [
        "### check the set with this\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print(stop_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "syoH5uCfYqCY"
      },
      "source": [
        "**a. Update your set with some stop words which are not needed for modeling, you think.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BhWXJ2M5YqCa",
        "colab": {}
      },
      "source": [
        "### YOUR CODE HERE ###\n",
        "#Hint in the paremeter '' input your stop_words\n",
        "stop_words.update([''])\n",
        "stop_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "njn1Qaw6YqCi"
      },
      "source": [
        "**b. Remove stop words from the tweet dataset with your own stop words set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bd82CqPJYqCk",
        "colab": {}
      },
      "source": [
        "### YOUR CODE HERE ###\n",
        "from nltk.tokenize import word_tokenize \n",
        "nltk.download(\"punkt\", quiet=True)\n",
        "word_tokens = []\n",
        "for text in text_en:\n",
        "    result = \"\"\n",
        "    row = word_tokenize(text )\n",
        "    \n",
        "    for w in row: \n",
        "        if w not in stop_words: \n",
        "            result = result +\" \" +str(w) \n",
        "            \n",
        "    word_tokens.append(result)\n",
        "\n",
        "word_tokens[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7YhWzL2pYqCw"
      },
      "source": [
        "### Problem 4-2 Convert the documents to a tf-idf matrix \n",
        "*TF-IDF* is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. Please refer to [this](http://mlwiki.org/index.php/TF-IDF)\n",
        "- Please use the results of 4-1\n",
        "- First, transform your tweets dataset into tf-idf matrix using the module below\n",
        "- Print your tf-idf matrix after converting it into *pandas.DataFrame* format\n",
        "- Set your own hyperparams."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DYOW_H9xYqCx",
        "colab": {}
      },
      "source": [
        "### scikit learn ###\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bqH48GWYYqC8",
        "colab": {}
      },
      "source": [
        "### YOUR CODE HERE ###\n",
        "tfidfv = TfidfVectorizer(min_df = 5,\n",
        "    max_df = 0.95,\n",
        "    max_features = 8000,\n",
        "    stop_words = 'english').fit(word_tokens)\n",
        "words = tfidfv.transform(word_tokens)\n",
        "#print(tfidfv.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rL8qMHj7YqDH"
      },
      "source": [
        "### Problem 4-3 Cluster the tf-idf matrix of tweets using the *K-Means* algorithm.\n",
        "- Set your own hyperparams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V02S6raGYqDI",
        "colab": {}
      },
      "source": [
        "### scikit learn ###\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import MiniBatchKMeans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PXYfHXZmYqDR"
      },
      "source": [
        "**a. Find an optimal value for the number of clusters using the *elbow method***\n",
        "- Draw the plot for SSD(Sum of the Squared Distance) of *K-Means* algorithm by the number of clusters, K, as shown in the example\n",
        "\n",
        "\n",
        "Example\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MKULaWwSYqDS",
        "colab": {}
      },
      "source": [
        "### YOUR CODE HERE ###\n",
        "sse = []\n",
        "for k in range(5, 105, 5):\n",
        "    sse.append(MiniBatchKMeans(n_clusters=k, init_size=1024, batch_size=2048, random_state=0).fit(words).inertia_)\n",
        "        \n",
        "f, ax = plt.subplots(figsize=(15, 6))\n",
        "ax.plot(x, sse, marker='^')\n",
        "ax.set_xlabel('Number of Clusters')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(x)\n",
        "ax.set_ylabel('Sum of Square Distance')\n",
        "ax.set_title('Elbow Method')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v-1RNcAyYqDY"
      },
      "source": [
        "**b. Based on the your plot above, pick a proper value for K and justify why.**\n",
        "\n",
        "It is OK that you can't find any proper value for K. If not, specify why these are not proper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "K19vzubQYqDe"
      },
      "source": [
        "**### YOUR DESCRIPTION HERE ###**\n",
        " - K=40 일때 Distnace 가 꾸준히 줄어들다가  그 이후 횡보하기 때문에 40일때가 적절하다고 하다고 판단된다.\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V7SHA-D1xGZW"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oKB5T0UwYqDf"
      },
      "source": [
        "### Problem 4-4 Visualize your clusters to evaluate the results of clustering analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C-v_w6uCYqDh"
      },
      "source": [
        "**a. Assgin the cluster index for each tweet with K=14**\n",
        "- In other words, predict the label for each example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bYLL_DcDt_Ko",
        "colab": {}
      },
      "source": [
        "# 14개의 그룹으로 나누는 K-Means 모델을 생성합니다\n",
        "model = MiniBatchKMeans(n_clusters=14, init_size=1024, batch_size=2048, random_state=20)\n",
        "model.fit(words)\n",
        "labels = model.predict(words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0HsffBYiYqDu"
      },
      "source": [
        "**b. Fill the function in below, which visualizes your clusters with PCA and TSNE**\n",
        "\n",
        "Both *PCA* and *TSNE* are the most common visualization techniques in the machine learning community. It is usually known that TSNE could capture the non-linear relationship between datapoints than PCA. For more detailed information, please refer to [this](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)\n",
        "- At first, this function randomly selects some samples for fitting PCA, depending on your device's capability.\n",
        "- Get the PCA matrices from the results of 4-2(tf-idf matrix) where the number of principal components is 2, each.\n",
        "- If you are uncomfortable with this function, you can make your own function to accomplish the same goal with this.\n",
        "\n",
        "Example\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9QwN9mRbYqDv",
        "colab": {}
      },
      "source": [
        "### visualization methods ###\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "### for plotting ###\n",
        "import matplotlib.cm as cm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5uhcywvAYqDy",
        "colab": {}
      },
      "source": [
        "def plot_tsne_pca(data, labels):\n",
        "    '''\n",
        "    This function plots the PCA and t-SNE on 2D plane.\n",
        "    args:\n",
        "        data: tf-idf weight matrix\n",
        "        labels: predictions from K-means\n",
        "    '''\n",
        "    # initial set up and random pick up samples\n",
        "    max_label = max(labels)\n",
        "    max_items = np.random.choice(range(data.shape[0]), size=2000, replace=False)\n",
        "    \n",
        "    ######################## YOUR CODE HERE ########################\n",
        "    '''\n",
        "    pca (2000 x 2) : extract 2 eigenvectors that have the most explained variance\n",
        "    tsne (2000 x 2) : feed the 50 principal components to t-SNE\n",
        "                     For TSNE, you can use the TSNE().fit_transform() function.\n",
        "    '''\n",
        "    \n",
        "    pca = PCA(n_components=2).fit_transform(data[max_items, :].todense())\n",
        "    tsne = TSNE().fit_transform(PCA(n_components=50).fit_transform(data[max_items,:].todense()))\n",
        "    \n",
        "    #################################################################\n",
        "    \n",
        "    # random pick certain size of data points for visiualization\n",
        "    idx = np.random.choice(range(pca.shape[0]), size=400, replace=False)\n",
        "    label_subset = labels[max_items]\n",
        "    label_subset = [cm.hsv(i/max_label) for i in label_subset[idx]]\n",
        "    \n",
        "    f, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
        "    \n",
        "    # plot PCA\n",
        "    ax[0].scatter(pca[idx, 0], pca[idx, 1], c=label_subset)\n",
        "    ax[0].set_title('PCA Cluster')\n",
        "    \n",
        "    # plot t-SNE\n",
        "    ax[1].scatter(tsne[idx, 0], tsne[idx, 1], c=label_subset)\n",
        "    ax[1].set_title('TSNE Cluster')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7fhoR_XRYqD7",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "### YOUR CODE HERE ###\n",
        "# plot PCA and t-SNE reduced data with above function\n",
        "plot_tsne_pca( words, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XOJ9ODFMYqEU"
      },
      "source": [
        "### Problem 4-5 Extract keywords from each clusters\n",
        "\n",
        "**a. get the top keywords from each cluster you've got from 4-4 (a.), based on the tf-idf matrix values**\n",
        "\n",
        "- Use the function in below or make your own function to achieve the same goal with this function.\n",
        "\n",
        "Example\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jhDk5kHGYqEY",
        "colab": {}
      },
      "source": [
        "def get_top_keywords(data, clusters, labels, n_terms=10):\n",
        "    '''\n",
        "    This function displays the top keywords based on tf-idf score.\n",
        "    data = tf-idf array\n",
        "    clusters = assigned cluster index\n",
        "    labels = vectorizer.get_feature_names()\n",
        "    n_terms = top n keywords\n",
        "    '''\n",
        "    #  based on predictions\n",
        "    df = pd.DataFrame(data.todense()).groupby(clusters).mean()\n",
        "    \n",
        "    # loop through each clusters and print top 10 score words\n",
        "    for i,r in df.iterrows():\n",
        "        print('\\nCluster {}'.format(i))\n",
        "        print(','.join([labels[t] for t in np.argsort(r)[-n_terms:]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "94C2Py4sYqEj",
        "colab": {}
      },
      "source": [
        "### YOUR CODE HERE ###\n",
        "get_top_keywords(words, labels, tfidfv.get_feature_names(), 10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TeUQ19MrYqEp"
      },
      "source": [
        "**b. According to the keywords selected from (a.), describe the differences between clusters or guess the topic of tweets in the specific cluster**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J-p5o0GDYqEq"
      },
      "source": [
        "**### YOUR DESCRIPTION HERE ###**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zbP91hJFYqEr"
      },
      "source": [
        "Cluster 1\n",
        "german,finance,state,dont,suicide,people,need,minister,said,know\n",
        "\n",
        "Cluster 2\n",
        "mental,support,people,public,thank,healthcare,care,workers,health,pandemic\n",
        "\n",
        "Cluster 3\n",
        "china,need,virusoutbreak,like,stayathomeandstaysafe,world,time,new,update,people\n",
        "\n",
        "- Cluster 1는 금융, 주, 자살, 도움, 장관의 단어들이 크러스터링 되고, \n",
        "  Cluster 2는 멘탈, 지원, 사람, 공적, 감사, 헬스케어, 케어 등 \n",
        "  \n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VXF4zbrsYqEt"
      },
      "source": [
        "### End of HW4 ###\n",
        "Thanks for all your works"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b2i_fWGlymkC",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}